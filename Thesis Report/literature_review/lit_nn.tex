\section{Computer vision using neural networks}

{\color{red} make better intro here!}

Computer vision is an extremely large field of study on its own, but only a part of the system being designed. The only concepts discussed are those which one must absolutely know for the completion of this project.

\subsection{Basic computer vision concepts}
Photo-based computer vision techniques tend to be able to differentiate between classes of objects by using a number of cascaded filters, with each layer of filters being able to detect more and more complex shapes in an image {\color{red} (cite)}. These filters are known as 'kernels', and tend to be implemented as square matrices which are convolved with the input image, a 2D array of pixel values.

As an example, Figure~\ref{fig:input_output_simple_filter} shows a simple image before (left) and after (right) being convolved with a vertical line finding kernel. {\color{red} make it clear that I made this!}

\begin{figure}[h!]%
    \centering
    \subfloat[][Input image]{\includegraphics[width=5cm]{literature_review/input_image}}%
    \qquad \qquad
    \subfloat[][Output Image]{\includegraphics[width=5cm]{literature_review/output_image}}%
    \caption{An image, before and after being convolved with a vertical line finder.}%
    \label{fig:input_output_simple_filter}%
\end{figure}

Note how the the vertical line in the left part of the input has been found (represented by bright pixels in the output image). The horizontal line on the bottom right has been rejected. The faint vertical line in the top right has been found, albeit faintly.

The exact same transformation is shown below, where the images have been replaced with their underlying matrices. The leftmost matrix is the pixel values of the input matrix, the middle is the kernel and the rightmost matrix is the output. \\ \\

\begin{table}[h!]
	\centering
	\begin{tabular}{ p{0.5cm} p{0.5cm} p{0.5cm} p{0.5cm} p{0.5cm} p{0.5cm} p{0.5cm} p{0.5cm} p{0.5cm} p{0.5cm} p{0.5cm} p{0.5cm} p{0.5cm} p{0.5cm} p{0.5cm}}
 5 & 232 & 180 & 212 & 180 &           &    &   &    &   & 0 & 255 & 0 & 200 & 0 \\
30 & 243 & 152 & 206 & 188 &           & -1 & 2 & -1 &   & 0 & 255 & 0 & 116 & 0 \\
32 & 255 & 210 & 190 & 190 & $\otimes$ & -1 & 2 & -1 & = & 0 & 255 & 67 & 0  & 0 \\
11 & 242 & 235 & 230 & 210 &           & -1 & 2 & -1 &   & 0 & 255 & 0 &  0  & 0 \\
90 & 240 & 130 &  20 &  30 &           &    &   &    &   & 0 & 255 & 0 &  0  & 0
	\end{tabular}
\end{table}

A larger, more interesting input image is shown in Figure~\ref{fig:image_kernel_demo} below.

% TODO: reference http://setosa.io/ev/image-kernels/
\begin{figure}[h!]
  \centering
  \includegraphics[width=\textwidth]{literature_review/image_kernel_demo}
  \caption{\label{fig:image_kernel_demo}A demonstration of image convolution on a photo of a human {\color{red} (cite)}.}
\end{figure}

The output image on the right hand side is the result of applying an edge-emphasizing filter. Note how the vertical lines around the shape of the face and nose are highlighted.

As an example of a complete system, consider a network of filters in which the first layer of kernels may detect vertical lines, angled lines, dots, and so on. These detections are then used as input matrices to the next layer, which will convolve them with a set of kernels to produce more complex detections. For example, two horizontal lines and two vertical lines may indicate the presence of a square in an image. A different configuration of lines may indicate a circle, and so on.

Carefully designing and combining these filters can ultimately result in the detection of an object. However, this method is incredibly time consuming and requires extensive domain-specific knowledge {\color{red} (cite)}. Even after the work has been put in to locate one type of object in a frame, locating another type of object could require the user to start from scratch.

\subsection{A brief introduction to machine learning}\
Hand selecting kernels may work for simple applications, but for more complex applications, automating the selection of these values may become necessary {\color{red} (cite)}. Machine learning algorithms (in which computers use datasets data 'learn' the parameters of a model) present a set of potential solutions to this problem {\color{red} (cite)}. For computer vision, this boils down to using known input-output pairs of images of objects and labels corresponding to the classes of objects in each image to optimize the kernel values in the network of filters.

{\color{red} talk a bit about neural networks (mainly fully connected layer type stuff) and include an image. mention the multi-layer perceptron. This part needs some work}

The standard method of doing this for images is through the use of Convolutional Neural Networks (CNNs), a class of large artificial neural networks {\color{red} (cite)}. They typically consist of layers of kernels (as discussed previously) along with an array of other operations borrowed from the fields of computer vision and machine learning.

CNNs are initialized with random values in their kernels and random values for the transformations which combine the outputs of their kernels - only their general architecture tends to be fixed by human practitioners {\color{red} (cite)}. These values are known as 'weights'. A summary of the method to get an optimal set of weights is as follows {\color{red} (cite)}:
%These values are known in the field as 'weights', while calculations are performed by 'neurons' - nodes in the CNN which perform an operation.

\begin{enumerate}
\item Initialize the CNN with a set structure and random weights.
\item Pass an image through the CNN, and take note of the output.
\item Get the error (the difference between the CNN output and the correct answer) and propagate it back through the CNN, adjusting the weights \emph{slightly} in such a way that that same input would be more likely to produce the correct output if run again. This is known as 'back-propagation'.
\item Repeat the first three steps a few hundred or thousand times until the performance stops increasing. It is helpful to use a large dataset consisting of a diverse set of images.
\end{enumerate}

This process is known as 'training'.

While it is possible that the CNN will act as an inefficient look-up table (mapping those specific inputs to their correct outputs, but giving the wrong result for a new image which it hasn't been exposed to {\color{red} (cite)}) it is hoped that the CNN will instead map any \emph{similar} input to the correct output. As an example, a correct training CNN will map any image which \emph{looks} like a cheetah (has the correct body structure, spots, ear location, etc) to an output which is numerically encoded to represent "cheetah". The process of taking an input and producing an output is known as 'inferencing'.

It should be noted that CNNs are nothing more than a mapping from a vector of input pixels (the image) to a vector which represents certain outputs (such as "cheetah", "no cheetah" or even the coordinates of a cheetah) {\color{red} (cite)}. Thus, in essence, this entire process is the solution to an optimisation problem: how to find the best set of weights to correctly map an input to the correct output.

CNNs simply require a few hundred or thousand labelled images and a few hours on a modern GPU {\color{red} (cite)}. There are many tips, tricks and open datasets which can be used to make this work easier.

\subsection{Key machine learning concepts and definitions}
Before getting lost in activation functions, dropout and the rest of the endless list of ideas in machine learning, it is worth considering the separation of jobs in modern data science.

Typically, there are one or more data scientists who design the neural network architecture, tune hyper-parameters (important parameters usually set by humans), publish results, and so on {\color{red} (cite)}. This requires a certain set of skills.

Next, an engineer with a separate skill set puts the very same neural network into production in order to achieve a goal - possibly after retraining it on another dataset. In this way there is often a clear separation between the people who create the tools (neural network architectures, training techniques, and so on), and the people who use the tools as part of their projects. Due to the computationally expensive nature of CNNs {\color{red} (cite)} using the tools effectively is often a great undertaking on its own.

To illustrate this with a topical example, a data scientist might create a CNN which effectively recognises humans, while an engineer would use that same unmodified neural network to calculate whether a self driving car is on a collision path with a pedestrian.

Since the project at hand doesn't necessitate a brand new architecture, the latter group is what will be focused on. Thus, only a few ideas need to be discussed.

\subsubsection{General terminology}

\textit{Machine learning, neural networks, deep learning, CNNs}: machine learning is the branch of computer science involved in fitting a model using data alone {\color{red} (cite)}. Neural networks are a subset of machine learning, specifying a class of architectures loosely inspired by the synapses in the nervous system of an organic brain {\color{red} (cite)}. Deep learning is the relatively recent practice of creating especially long neural networks in order to represent more complex mappings {\color{red} (cite)}. Finally, CNNs are a subset of deep learning, in which the synapses of a deep neural network are kernels which perform convolution operations {\color{red} (cite)}.

\textit{Image classification vs object detection:} an image classifier indicates \emph{what} objects are in an image. An object detector finds what objects are in the image, and also \emph{where} in the image they can be found {\color{red} (cite)}. One might think to perform object detection using an image classification CNN by dividing an input image into a grid of smaller overlapping images, and then passing these smaller images to the CNN. By noting which objects are detected in each image, one could estimate where in the original image the object can be found. Interestingly, this is almost exactly what object detectors do internally! {\color{red} (cite)} In fact, ignoring some minor details like 'strides' and 'pooling', it's almost exactly what convolution with a kernel does. Newer object detection architectures are internally a lot more sophisticated than this but operate with the same intuition.

\textit{Bounding box:} also known as an anchor, a bounding box is a pair of points which determine a box around a detected object. Two $(x, y)$ pairs of numbers are usually part of the output of an object detection neural network.

{\Huge \color{red} show image from pi cam with box around objects}

\textit{Feature extractor:} data scientists sometimes concatenate an well-designed existing neural network architecture with another architecture to produce a new, larger model which performs an overall more complex task {\color{red} (cite)}. Thus, the role of the first part of the new model is to extract useful features for the second part to process. In this way, neural networks can be a bit like Lego.

\textit{Transfer learning:} this refers to the practice in which the weights of one neural network are copied to another network {\color{red} (cite)}. This enables problems to be solved using much less data than neural networks ordinarily require, as the weights are already quite good before the retraining process. The idea is that the early layers are likely to have been trained to find very general features in the image (such as lines, circles, eyes, tails, etc). These are often relevant to a wide variety of problems. This is why a trained network will often have its training dataset specified - the content of the training dataset is a good indicator of whether transfer learning would help in a different problem.

%\subsubsection{Training-specific terminology}
%{\color{red} fill this stuff in? or maybe this whole terminology section isn't really appropriate??}

%\textit{Epoch:}

%\textit{Batch size:}

%\textit{Loss:}

%\textit{Optimizers:}






\subsection{A brief introduction to neural accelerator sticks}
CNNs tend to require a large number of operations to produce an output {\color{red} (cite)}. The exact amount of computation required depends on the neural network architecture, though even relatively small object detectors generally require on the order of millions of multiply-accumulate instructions with significant amounts of data being loaded to and from the various levels of cache. Combined with the slower processor on many mobile computer platforms, the neural network would not be able to run in real time{\color{red} (cite)}. As an example of the inference times involved, a cutting edge object detection network designed for use on mobile devices typically requires around one full second to infer a result on the CPU of a Raspberry Pi model 3.

Fortunately, this work can be easily parallelized over multiple processing cores {\color{red} (cite)}. To this end, there are three types of parallelization which tend to occur: during the application of kernels, during matrix multiplication and in parts of the neural network where the data flow is naturally parallel. This work is a perfect fit for GPUs, which contain a large number of processing cores designed for such tasks. However, since GPU support for CNNs on non-NVIDIA devices is lacking, other means had to be investigated.

% https://petewarden.com/2014/08/07/how-to-optimize-raspberry-pi-code-using-its-gpu/
% https://rpiplayground.wordpress.com/2014/05/03/hacking-the-gpu-for-fun-and-profit-pt-1/

%\textit{For the rest of this chapter, assume any obscure words have been patented by Intel$^{^{\ TM}}$ or one of its subsidiaries, and add a $TM$.}

To solve this problem, some chip makers have started to produce 'neural accelerators' - devices which use parallelization to decrease the inference time of neural network {\color{red} (cite)}. One such as the Movidius Neural Compute Stick (NCS), a specialized neural network accelerator which plugs into an underpowered device (such a Raspberry Pi) to offload and speed up inferencing time. Its purpose-built processing cores typically result in inference speed increases of between $700\%$ and $1000\%$. This is the benefit of being purpose built for CNNs.

The operation of the Movidius NCS is as follows: first, the user designs and trains their neural network. Next, they compile it down to a specific format optimized for the NCS and upload it to the device. From then on, the user can send preprocessed images to the NCS, wait for the inference to occur, and then retrieve the result.

The actual processing is done using Intel's Myriad 2 Vision Processing Unit (VPU), which makes use of 12 specially designed processing cores named 'shaves'.

%\begin{figure}[h!]
%  \centering
%  \includegraphics[width=0.4\textwidth]{literature_review/NCS_internals}
%  \caption{\label{fig:NCS_internals} Internals of the Movidius Neural Compute Stick.}
%\end{figure}


\subsection{Comparison of neural network architectures}
Not all neural network architectures are equal. Noteworthy differences between models generally include {\color{red} (cite)},

\begin{itemize}
	\item Classification accuracy,
	\item Inference speed, which is a function of
	\begin{itemize}
		\item The number of operations (processing) and
		\item The number of weights (data retrieval)
	\end{itemize}
	\item The amount of data required for training/fine tuning,
	\item The existence (or lack) of pretrained models in each specific deep learning framework,
	\item Whether recent innovations in the field have been included, and
	\item The underlying method in which objects are detected and localised within the image
\end{itemize}

{\Huge \color{red} how should i finish this off? ack! maybe remove this section??}

Newer neural networks often take ideas from older architectures. Sometimes, they even include all or most of a previous architecture as part of the design of the new model. An example of this is MobileNet - a class of image recongition architectures designed at Google, aimed to run quickly on modern mobile devices (such as newer smartphones). A common practice is to train MobileNet to classify objects on a given dataset, then remove the final layers, concatenate it with another model (with MobileNet acting as a feature extractor) and end up with an object detector.


% TODO:
% https://towardsdatascience.com/understanding-ssd-multibox-real-time-object-detection-in-deep-learning-495ef744fab
% https://medium.com/@smallfishbigsea/understand-ssd-and-implement-your-own-caa3232cd6ad
% after having spent significant amounts of time understanding the operation of a few types of neural network architectures, it can confidantly be said that there is little need to really understand the underlying operation at a deep level if you're not going to extend the network yourself. simply a different field. rather, then, read up on speed, accuracy and training requirements


{\color{red}
Neil's notes:

Neural networks have been used for solving problems in many applications [cite]. Noteable examples include:

bullets.

For the specific problem of object detection/tracking/whatever, the following notable examples have been considered:

talk about papers/comparisons.

"type of network" was selected as the network of choice due to it's performance of  in <paper here> [cite].
}
