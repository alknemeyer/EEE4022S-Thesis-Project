\chapter{EKF Modelling and Design}

{\Large \color{red} write intro to this section}

\section{System modelling}
{\color{red} talk about KF vs EKF choice!}

An EKF was used to estimate the state of the tracked object as it moves around. Since the camera system generally rotates around an axis and there is no reliable estimate of the distance from the camera to the object being tracked, it was decided that the state would be measured in angular coordinates.

A model of the dynamics of the tracked object had to be chosen for the EKF to operate optimally. However, there is no possible "correct" model for the movements of the object being tracked (for example, a cheetah won't stick to the rule of moving at a constant velocity) and no way to estimate the disturbances $u$ which may effect its decision to change its velocity. Thus, it was decided that the model would make the incorrect assumption the the object would either maintain a constant velocity, constant acceleration of exponentially decaying acceleration. These three approaches needed to be tested, with the best being used in the final model.

This resulted in the global angular position, velocity and acceleration of the tracked object being estimated. These were stored in the state matrix $x$ as,

\[ \underline{x} = \begin{bmatrix} \theta \\ \dot{\theta} \\ \ddot{\theta} \end{bmatrix} \]

These states are related through integration, where $\theta = \int{\dot{\theta} dt} = \int{\int{\ddot{\theta} dt}dt}$. They can be approximated using the discrete integration formulae, $\dot{\theta}_k = \dot{\theta}_{k-1} + \Delta T \ddot{\theta}_{k-1}$ and $\theta_k = \theta_{k-1} + \Delta T \dot{\theta}_{k-1}$. The prediction matrix was thus chosen as,

\[ F = \begin{bmatrix} 1 & \Delta T & 0 \\
                       0 & 1 & \Delta T \\
					   0 & 0 & \beta
		\end{bmatrix} \]

where $\beta$ is a factor which could be $\beta = 1$ for constant acceleration ($a_i = a_{i-1}$ in the predict stage), $0 < \beta < 1$ for decaying accelerating (for example, $a_i = 0.9 a_{i-1}$) or $\beta = 0$ for no acceleration. This was done to ensure quick and easy prototyping.

Next, the sensor needed to be added. The neural network returns the coordinates of the position of the object, measured in pixels. For the sake of modularity and simpler math, the pixel range of \pyth{pixel} $\in$ \pyth{[0, 299]} along a single axis was normalized to a value \pyth{pixel_norm} $\in$ \pyth{[-1, 1]}, with a pixel value of zero corresponding to the centre of the image. An illustration of this is shown below, in Figure~\ref{fig:pixel_to_angle}.

\begin{figure}[h!]
  \centering
  \includegraphics[width=\textwidth]{methodology/pixel_to_angle2}
  \caption{\label{fig:pixel_to_angle} Diagram showing the mapping from normalized pixel to angle.}
\end{figure}

Note that the pixel coordinate of the detected object is given by \pyth{x}, the angle to the object is \pyth{phi} ($\phi$) and the field of view of the camera is \pyth{theta} ($\theta$). Letting the distance between the camera and the plane at the closest point be noted as $L$, one can convert between pixel and angle as follows:

\[ L = \frac{1}{\tan{\theta/2}} = \frac{x}{\tan{\phi}} \]
Resulting in,
\[ \phi = \tan^{-1}{\left( \frac{\tan{ \theta/2 }}{x} \right) }
\qquad or
\qquad x = \frac{\tan{\phi}}{\tan{\theta/2}} \]

Thus, the mapping from pixel to angle is nonlinear. This is why an EKF was chosen instead of a linear Kalman Filter - a one pixel error in the middle of the image will propagate to a different angular error than a one pixel error at the edge. The Jacobian could then be calculated as,

\[ \frac{\delta h}{\delta x} = \frac{\delta}{\delta x} \left[ \frac{\tan{\phi}}{\tan{\theta/2}} \right] = \frac{1 + \tan{(\phi)}^2}{\tan{\theta/2}} \]

Finally, the $Q$ and $R$ matrices needed to be determined. In most systems, these can be calculated after careful modelling of the objects dynamics and consultation with component datasheets. However, in this system, the $Q$ matrix determined for an object which is close to the camera would be wildly inaccurate when the object is farther away, since distance affects the perceived angular movement of an object moving in a straight line. Since there is no distance estimate, this could not be reliably factored into a constantly changing $Q$. In addition, the neural network has no obvious value to use for the sensor noise, $R$.

Thus, it was decided that these would be left as parameters to be tuned using recorded data.

\section{Implementation of the EKF}

While python wasn't designed for purely numerical programming (such as matrix multiplication), when the correct packages are used it can approach C-level speeds. \pyth{numpy} is one such package - it provides python bindings to C arrays, resulting in incredible speedups over regular python arrays.

Thus, the Kalman Filter and Extended Kalman Filter were implemented as python classes which contain \pyth{numpy} matrices. A snippet of the implementation can be seen below, while the rest of the code can be found on the author's GitHub page. \\

\begin{python}
class KalmanFilter():
    def __init__(self, Ts, Q, R, a=1):
        self.Ts = Ts
        self.x = np.matrix([[0],  # position
                            [0],  # velocity
                            [0]]) # acceleration
        self.P = np.matrix(np.eye(3))
        self.F = np.matrix([[1, Ts,  0],
                            [0,  1, Ts],
                            [0,  0,  a]])
        self.Q = np.matrix([[Q*(Ts**2),    0, 0],
                            [        0, Q*Ts, 0],
                            [        0,    0, Q]])
\end{python}

\subsection{KF/EKF stuffs}
{\Large \color{red} talk about how the EKF is basically the same code as the KF, with the exception that the $h$ and $\frac{\delta h}{\delta x}$ functions are modified.}

\subsection{EKF timing issues}
{\Large \color{red} talk about how I have to continuously change $F$ and $Q$ to account for the changing timing in the system}

\subsection{Tuning the process noise and sensor uncertainty}
The next step was to find and set reasonable values for the process noise $Q$ and sensor uncertainty $R$. This was doing by running the computer vision algorithm while a human walked and ran around the lab. The results of this were saved to a file, allowing for easy offline simulation. A plot of the raw updates from the neural network are shown below, in Figure~\ref{fig:raw_nn_results}.

\begin{figure}[h!]
  \centering
  \includegraphics[width=\textwidth]{methodology/raw_nn_results}
  \caption{\label{fig:raw_nn_results} A plot showing the outputs of the neural network, and the timing thereof.}
\end{figure}

Note how the time between inferences from the neural network is not constant - while all of the calculations in the neural network are the same in each run, non-constant run times are the natural consequence of running a control loop on a non real time operating system and using a programming language which stops occasionally for garbage collection.

In addition, there are times when a few hundred milliseconds pass without receiving a new update - this occurs when the neural network can't find an object in the frame. It is worth noting that the neural network is in fact very good at locating objects, and misses them infrequently. However, the frequent and large gaps in object locations was added in order to simulate tracking an object which may disappear behind a bush or some other obstacle.

Next, a small script was made to speed up the trial-and-error process of comparing the advantages and disadvantages of a variety of $Q$ and $R$ values. In addition, the possibility of switching to a Kalman Filter and changing the acceleration modifying parameter $\beta$ were added. In the end, values were chosen to satisfy the following two criteria:

\begin{enumerate}
\item Trust the updates from the neural network more than the model itself. This was decided on because the network was known to accurately find objects in a frame, whereas the constant velocity or acceleration model was \emph{known} to be incorrect.
\item Make reasonable predictions in the event that no updates appeared from the neural network. 'Reasonable' meant that, if the data indicated that the object was moving in a certain direction at a certain velocity, it is reasonable that the object would keep moving in that direction and at that velocity.
\end{enumerate}

A plot of the interactive process and selected paramters is shown below, in Figure~\ref{fig:tuning_Q_R}.

\begin{figure}[h!]
  \centering
  \includegraphics[width=\textwidth]{methodology/tuning_Q_R}
  \caption{\label{fig:tuning_Q_R} A plot of the output of the EKF given stem plot data as input.}
\end{figure}

As can be seen in Figure~\ref{fig:tuning_Q_R}, choosing $Q = 0.5$, $R = 0.005$ and $a = 0.95$ resulted in an EKF which closely follows and trusts the updates from the neural network, whilst making reasonable predictions in the event that no data was received. It is worth noting that the input data was more sinusoidal than 'roughly constant velocity', which was the main assumption behind the dynamics of the model. This was due to a lack of running space in the mechatronics engineering lab - once wall has been reached, there's no place to go except backwards.

This concluded the implementation of the Extended Kalman Filter.
