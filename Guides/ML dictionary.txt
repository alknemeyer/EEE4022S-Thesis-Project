Sample:
    One element of a dataset (eg one image)


Minibatch:
    A set of samples
    The larger the batch, the better the approximation; however, it is also true that the batch will take longer to process and will still result in only one update. For inference (evaluate/predict), it is recommended to pick a batch size that is as large as you can afford without going out of memory (since larger batches will usually result in faster evaluating/prediction)
    The main advantage: train on images in groups to properly utilize parallel GPU processes
    Not much use in doing more than 64 images per batch (allegedly)


Epoch:
    An arbitrary cutoff, generally defined as "one pass over the entire dataset", used to separate training into distinct phases, which is useful for logging and periodic evaluation.


Loss and Accuracy:
    Accuracy is the ratio of correct prediction to the total number of predictions

    In machine learning the loss function or cost function is representing the price paid for inaccuracy of predictions
    The loss associated with one example in binary classification is given by:
        -(y * log(p) + (1-y) * log (1-p))
    where y is the true label of x and p is the probability predicted by our model that the label is 1


Confusion Matrix Terminology:
    true positives (TP): predict yes, actual yes
    true negatives (TN): predict no, actual no
    false positives (FP): predict yes, actual no (aka "Type I error")
    false negatives (FN): predict no, actual yes (aka "Type II error")

    Accuracy = (TP + TN)/total
        overall, how often is classifier correct?
    Misclassification Rate = (FP + FN)/total
        overall, how often is it wrong?
        = 1 - accuracy
        aka "error rate"
    True Positive Rate = TP/actual yes
        When it's actually yes, how often does it predict yes?
        aka "sensitivity" or "recall"
    False Positive Rate = FP/actual no
        When it's actually no, how often does it predict yes?
    Specificity = TN/actual no
        When it's actually no, how often does it predict no?
        = 1 - False Positive Rate
    Precision = TP/predicted yes
        When it predicts yes, how often is it correct?
    Prevalence = actual yes/total
        How often does the yes condition actually occur in our sample?


Other Testing Terminology:
    Positive Predictive Value:
        Very similar to precision, except that it takes prevalence into account.
        In the case where the classes are perfectly balanced (meaning the prevalence is 50%), the positive predictive value (PPV) is equivalent to precision
    Null Error Rate:
        This is how often you would be wrong if you always predicted the majority class. This can be a useful baseline metric to compare your classifier against. However, the best classifier for a particular application will sometimes have a higher error rate than the null error rate, as demonstrated by the Accuracy Paradox.
    Cohen's Kappa:
        This is essentially a measure of how well the classifier performed as compared to how well it would have performed simply by chance. In other words, a model will have a high Kappa score if there is a big difference between the accuracy and the null error rate
    F Score:
        This is a weighted average of the true positive rate (recall) and precision
    ROC Curve:
        This is a commonly used graph that summarizes the performance of a classifier over all possible thresholds. It is generated by plotting the True Positive Rate (y-axis) against the False Positive Rate (x-axis) as you vary the threshold for assigning observations to a given class


Bias and Variance:
    High bias
        underfits the data --> maps poorly
    High variance
        overfits the data --> doesn't generalise


Train + test + validation sets:
    Train:
        Used to train a given model - pretty straightforward
    
    Validation:
        Used to choose between models (eg random forest vs neural network)
    
    Test:
        Tells you how well you've actually done. If youâ€™ve tried out a lot of different models, you may get one that does well on your validation set just by chance, and having a test set helps make sure that is not the case

    If train loss << validation loss, overfitting is likely happening

    When sorting data into train/test/validation groups, carefully think about how it should be done. For things like images, random subsets is probably fine, but you can't for time series data (the model might just look at the dates directly before and after and interpolate, when in reality you need the model to extrapolate). So, choose a continuous section with the latest dates as your validation set. Another example: say you're guessing someone's age from a picture, and you have multiple pictures of each person. If the same person appears in both train and validation sets, the model is given the oppurtunity to memorise the person's age from the train set and just find the same person during the test/validation sets. So, be careful with how you segment your data

    Article:
        http://www.fast.ai/2017/11/13/validation-sets/


Fine tuning:
    Modify a pretrained neural network to work with a new but similar dataset
    The idea is to utilize the existing structure + filtering/convolving/etc effects in the front couple layers, and just change the last layer(s) to output results related to the new dataset


One hot encoding:
    (Correct) labels represented by an array which contains a single 1 in the position corresponding to that category
    Common approach to encoding categorial variables
    eg. weather E ['sunny', 'rainy', 'cloudy'] ==> [1 0 0] for 'sunny', [0 1 0] for 'rainy', etc


Max pooling:
    Down-sample an input representation, reducing its dimensionality
    Usually just take the '''max''' of each section that the pic is divided into
    Eg: given a 4x4 image, max-pooling over 2x2 subsections of the image would output a 2x2 image, where each pixel is the largest pixel in each of the 4 2x2 areas in the original image
    --> reduces overfitting + computational cost
    --> 'forces' the nn to look into larger areas of the image at a time (eg identify cats and dogs instead of fur and noses)
    --> to make up for the information lost upon pooling, increase the number of filters in subsequent convolutional layers


Tip:
    If augmenting data with eg. shift/rotate, make sure to augment the bounding box stuff


Dropout: ????????


Activation: ??????
    ReLU, etc etc


Optimisers: ?????????
    ADAM, SGD, etc
    Stochastic Gradient Descent (SGD):
        Take derivative ????????????????????????????????????


Loss functions: ????????
    Cross entropy:
        sum(etc etc)


To add:
    Bias variables
    Convolution
    Recurrent Neural Networks (RNN)
    Convolutional Neural Networks (CNN)
