{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classifier\n",
    "\n",
    "Contains code for the `ImageClassifer` class, which loads a neural network on the Movidius NCS, takes pictures using a pi cam, passes the (preprocessed) pics through the stick and decodes the result into a bounding box around the category of your choice.\n",
    "\n",
    "https://picamera.readthedocs.io/en/latest/fov.html\n",
    "\n",
    "Main workflow when importing:\n",
    "1. `import image_classifier`\n",
    "2. `IC = image_classifier.ImageClassifier()`\n",
    "3. Repeat:\n",
    "    - `bb, bb_scaled = IC.get_result()`\n",
    "    - `if bb == -1:`\n",
    "        - `    pass`\n",
    "    - `else:`\n",
    "        - `    (x1, y1), (x2, y2) = bb  # do stuff`\n",
    "\n",
    "---\n",
    "\n",
    "TODO: Much of the code comes from the Movidius GitHub repo, and should be attributed properly!\n",
    "\n",
    "---\n",
    "\n",
    "I find development using a notebook to be quite a bit easier than developing using a regular python file. Unfortunately, you can't import a `.ipynb` as a module. So, here's the workflow:\n",
    "1. Use this file to understand the code and make changes\n",
    "2. When you want to commit a change, click `Kernal > Restart and Clear Output` to remove your outputs + make the file a bit smaller (shows up as fewer lines in the git commit)\n",
    "3. Run the command `jupyter nbconvert --to=python image_classifier.ipynb` to generate a `.py` file which can be imported as a module. Just make sure to remove your debugging code beforehand!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, multiprocessing, numpy as np\n",
    "import picamera, picamera.array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deserialize the SSD output to a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ****************************************************************************\n",
    "# Copyright(c) 2017 Intel Corporation. \n",
    "# License: MIT See LICENSE file in root directory.\n",
    "# ****************************************************************************\n",
    "\n",
    "# Utilities to help deserialize the output list from\n",
    "# Intel® Movidius™ Neural Compute Stick (NCS)\n",
    "def deserialize_ssd(output, shape, confidance_threshold):\n",
    "    \"\"\"---- Deserialize the output from an SSD based network ----\n",
    "    \n",
    "    @param output The NCS returns a list/array in this structure:\n",
    "        First float16: Number of detections\n",
    "        Next 6 values: Unused\n",
    "        Next consecutive batch of 7 values: Detection values\n",
    "          0: Image ID (always 0)\n",
    "          1: Class ID (index into labels.txt)\n",
    "          2: Detection score\n",
    "          3: Box left coordinate (x1) - scaled value between 0 & 1\n",
    "          4: Box top coordinate (y1) - scaled value between 0 & 1\n",
    "          5: Box right coordinate (x2) - scaled value between 0 & 1\n",
    "          6: Box bottom coordinate (y2) - scaled value between 0 & 1\n",
    "\n",
    "    @return output_dict A Python dictionary with the following keys:\n",
    "        output_dict['num_detections'] = Total number of valid detections\n",
    "        output_dict['detection_classes_<X>'] = Class ID of the detected object\n",
    "        output_dict['detection_scores_<X>'] = Percentage of the confidance\n",
    "        output_dict['detection_boxes_<X>'] = A list of 2 tuples [(x1, y1) (x2, y2)]\n",
    "        Where <X> is a zero-index count of num_detections\n",
    "    \"\"\"\n",
    "\n",
    "    output_dict = {}                # Dictionary where the deserialized output will be stored\n",
    "    height, width = shape  # Extract the original image's shape\n",
    "    channel = 3\n",
    "    output_dict['num_detections'] = int(output[0])  # Total number of detections\n",
    "    num_valid_detections = 0\n",
    "\n",
    "    for detection in range(output_dict['num_detections']):\n",
    "        base_index = 7 + (7 * detection)  # Skip the first 7 values\n",
    "\n",
    "        if (output[base_index + 2] > confidance_threshold):\n",
    "            output_dict['detection_classes_' + str(num_valid_detections)] = int(output[base_index + 1])\n",
    "            output_dict['detection_scores_' + str(num_valid_detections)] = int(output[base_index + 2] * 100)\n",
    "\n",
    "            x = [int(output[base_index + 3] * width), int(output[base_index + 5] * width)]\n",
    "            y = [int(output[base_index + 4] * height), int(output[base_index + 6] * height)]\n",
    "\n",
    "            output_dict['detection_boxes_' + str(num_valid_detections)] = list(zip(y, x))\n",
    "\n",
    "            num_valid_detections += 1\n",
    "\n",
    "    # Update total number of detections to valid detections\n",
    "    output_dict['num_detections'] = int(num_valid_detections)\n",
    "\n",
    "    return output_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils for conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_dict_to_bb_and_angles(output_dict, class_of_interest, camera_resolution, camera_FOV):\n",
    "    for i in range(output_dict['num_detections']):\n",
    "        if (output_dict.get('detection_classes_%i' % i) == class_of_interest):\n",
    "            (y1, x1) = output_dict.get('detection_boxes_' + str(i))[0]\n",
    "            (y2, x2) = output_dict.get('detection_boxes_' + str(i))[1]\n",
    "            bb = (x1, y1), (x2, y2)\n",
    "\n",
    "            w, h = camera_resolution\n",
    "            cam_width = camera_FOV[0]\n",
    "            cam_height = camera_FOV[1]\n",
    "            x1_angle = pixel_to_angle(x1, w, cam_width)\n",
    "            x2_angle = pixel_to_angle(x2, w, cam_width)\n",
    "            y1_angle = pixel_to_angle(y1, h, cam_height)\n",
    "            y2_angle = pixel_to_angle(y2, h, cam_height)\n",
    "            bb_angles = ((x1_angle, y1_angle), (x2_angle, y2_angle))\n",
    "\n",
    "            return bb, bb_angles\n",
    "    return -1, -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pixel_to_angle(pixel, img_size, cam_angle_deg):\n",
    "    \"\"\" convert a pixel value to an angle in degrees\n",
    "    inputs:\n",
    "        pixel:              a value from 0 to img_size\n",
    "        img_size:           the total number of pixels along that axis of the image\n",
    "        cam_angle_deg:      the angular width of camera in degrees\n",
    "        \n",
    "    output:\n",
    "        angle:      0deg = when pixel is directly ahead, positive/negative = right/left of center\n",
    "    \"\"\"\n",
    "    norm_pixel = (2*pixel/img_size) - 1  # now in range [-1, 1]\n",
    "    angle = np.arctan(norm_pixel * np.tan(np.deg2rad(cam_angle_deg/2)))\n",
    "    return np.rad2deg(angle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The ImageClassifier class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def picam_streamer(image_queue, e, # for multiprocessing\n",
    "                  camera_resolution, colourmode,\n",
    "                  nn_shape, scale, mean,\n",
    "                  debug):\n",
    "\n",
    "    if debug: print('picam_streamer: initialising camera')\n",
    "    with picamera.PiCamera(resolution=camera_resolution, framerate=90) as camera:\n",
    "        frame = picamera.array.PiRGBArray(camera, size=nn_shape)\n",
    "        cont_capture = camera.capture_continuous(frame, colourmode,\n",
    "                                                 resize=nn_shape,\n",
    "                                                 use_video_port=True)# use GPU for resizing\n",
    "\n",
    "        next(cont_capture)  # get the next frame in the continuous capture\n",
    "        \n",
    "        while True:\n",
    "            if debug: t = time.time()\n",
    "\n",
    "            if e.is_set():\n",
    "                print('picam_streamer: shutting down')\n",
    "                del cont_capture\n",
    "                del frame\n",
    "                break\n",
    "            \n",
    "            frame.seek(0)\n",
    "            frame.truncate(0)\n",
    "            next(cont_capture)\n",
    "            photo_time = time.time()\n",
    "            if debug: print('picam_streamer: time to capture photo: %d [ms]' % ((time.time()-t)*1000))\n",
    "            \n",
    "            if debug: _t = time.time()\n",
    "            preprocessed_img = (frame.array - np.float32(mean)) * np.float32(scale)\n",
    "            if debug: print('picam_streamer: preprocessing time: %d [ms]' % ((time.time()-_t)*1000))\n",
    "            \n",
    "            # wait until the next subprocess is ready to receive stuff\n",
    "            while not image_queue.empty():\n",
    "                # but if a second have passed without the buffer being cleared,\n",
    "                # it's probably because the other process has shut down\n",
    "                # so, shut this one down as well\n",
    "                if time.time() > photo_time + 1:\n",
    "                    break\n",
    "                pass\n",
    "            \n",
    "            image_queue.put(preprocessed_img)\n",
    "            image_queue.put(photo_time)\n",
    "            if debug: print('picam_streamer: total loop time: %d [ms]' % ((time.time()-t)*1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_infer(image_queue, dict_queue, e, # for multiprocessing\n",
    "             graph_filename,\n",
    "             nn_shape,\n",
    "             debug):\n",
    "\n",
    "    if debug: print('nn_infer: importing MVNC library and opening NCS device')\n",
    "    import mvnc.mvncapi as mvnc  # have to import the library here to get the thing to work, for some reason\n",
    "    \n",
    "    ## Start of Movidius code:\n",
    "    devices = mvnc.enumerate_devices()\n",
    "    if len(devices) == 0:\n",
    "        print('No NCS devices found')\n",
    "        exit()\n",
    "\n",
    "    # Get a handle to the first enumerated device and open it\n",
    "    device = mvnc.Device(devices[0])\n",
    "    device.open()\n",
    "\n",
    "    # Read the graph file into a buffer\n",
    "    with open(graph_filename, mode='rb') as f:\n",
    "        blob = f.read()\n",
    "\n",
    "    # Load the graph buffer into the NCS\n",
    "    graph = mvnc.Graph(graph_filename)\n",
    "    fifo_in, fifo_out = graph.allocate_with_fifos(device, blob)\n",
    "    ## End of Movidius code\n",
    "\n",
    "    while True:\n",
    "        if debug: t = time.time()\n",
    "            \n",
    "        if e.is_set():\n",
    "            print('nn_infer: shutting down')\n",
    "            break\n",
    "\n",
    "        preprocessed_img = image_queue.get()\n",
    "        photo_time = image_queue.get()\n",
    "\n",
    "        if debug: _t = time.time()\n",
    "        graph.queue_inference_with_fifo_elem(fifo_in, fifo_out, preprocessed_img, None)\n",
    "        if debug: print('nn_infer: queueing time: %d [ms]' % ((time.time()-_t)*1000))\n",
    "\n",
    "        ## Start of Movidius code:\n",
    "        if debug: _t = time.time()\n",
    "        output, userobj = fifo_out.read_elem()\n",
    "        if debug: print('nn_infer: result retrieval time: %d [ms]' % ((time.time()-_t)*1000))\n",
    "\n",
    "        # Deserialize the output into a python dictionary\n",
    "        confidance_threshold = 0.6\n",
    "        output_dict = deserialize_ssd(output, nn_shape, confidance_threshold)\n",
    "        ## End of Movidius code\n",
    "\n",
    "        # don't let the queue increase too much\n",
    "#         if dict_queue.qsize() > 2:\n",
    "#             dict_queue.get()\n",
    "        \n",
    "        output_dict['photo_time'] = photo_time\n",
    "        dict_queue.put(output_dict)\n",
    "        if debug: print('nn_infer: qsize =', dict_queue.qsize())\n",
    "        if debug: print('nn_infer: loop time: %d [ms]' % ((time.time()-t)*1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageClassifier():\n",
    "    def __init__(self,\n",
    "                 graph_filename='graph',\n",
    "                 label_filename='categories.txt',  # must correspond to the specific network\n",
    "                 class_of_interest='person',\n",
    "                 colourmode='rgb',\n",
    "                 camera_resolution=(1280, 720),# (width, height)\n",
    "                 nn_shape=(300, 300),          # (width, height)\n",
    "                 camera_FOV=(62.2, 48.8),      # (width, height)):\n",
    "                 mean=(127.5, 127.5, 127.5),   # depends on the colourmode\n",
    "                 scale=0.00789,                # = 1/127\n",
    "                 debug=False):\n",
    "        \n",
    "        labels = [line.rstrip('\\n') for line in open(label_filename) if line != 'classes\\n']\n",
    "        self.class_of_interest = labels.index(class_of_interest) # note conversion from string to ID (int)\n",
    "        self.nn_shape = nn_shape\n",
    "        self.camera_FOV = camera_FOV\n",
    "\n",
    "        self.image_queue = multiprocessing.Queue()\n",
    "        self.dict_queue = multiprocessing.Queue()\n",
    "        self.e = multiprocessing.Event()\n",
    "        \n",
    "        self.infer_process = multiprocessing.Process(\n",
    "                                                target=nn_infer,\n",
    "                                                args=(self.image_queue, self.dict_queue, self.e,\n",
    "                                                      graph_filename,\n",
    "                                                      nn_shape,\n",
    "                                                      debug))\n",
    "        \n",
    "        self.photo_process = multiprocessing.Process(\n",
    "                                                target=picam_streamer,\n",
    "                                                args=(self.image_queue, self.e,\n",
    "                                                      camera_resolution, colourmode,\n",
    "                                                      nn_shape, scale, mean,\n",
    "                                                      debug))\n",
    "\n",
    "        self.infer_process.start()\n",
    "        self.photo_process.start()\n",
    "        if debug: print('ImageClassifier: started child processes')\n",
    "    \n",
    "    def get_result(self, debug=False):\n",
    "        if debug: print('ImageClassifier: getting output dict')\n",
    "        output_dict = self.dict_queue.get()\n",
    "        bb, bb_angles = output_dict_to_bb_and_angles(output_dict,\n",
    "                                                     self.class_of_interest,\n",
    "                                                     self.nn_shape,\n",
    "                                                     self.camera_FOV)\n",
    "        return bb, bb_angles, output_dict['photo_time']\n",
    "    \n",
    "    def close(self): self.__del__()\n",
    "    \n",
    "    def __del__(self):\n",
    "        self.e.set()  # setting the flag signals the other processes to shut down\n",
    "        time.sleep(2)\n",
    "        self.image_queue.close()\n",
    "        self.dict_queue.close()\n",
    "        del self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# IC = ImageClassifier(graph_filename='../Models/MobileNet_SSD_caffe/graph',\n",
    "#                      label_filename='../Models/MobileNet_SSD_caffe/categories.txt',\n",
    "#                      camera_resolution=(1640,922),#(1280,720),\n",
    "#                      debug=False)\n",
    "\n",
    "# bb_arr = []\n",
    "# bb_angles_arr = []\n",
    "# t_start = time.time()\n",
    "# time_arr = []\n",
    "\n",
    "# while True:\n",
    "#     t = time.time()\n",
    "#     bb, bb_angles = IC.get_result()\n",
    "#     bb_arr.append(bb)\n",
    "#     bb_angles_arr.append(bb_angles)\n",
    "# #     _ = IC.get_result()\n",
    "# #     print('MAIN: loop time: %d [ms]' % ((time.time() - t)*1000))\n",
    "#     time_arr.append(time.time() - t_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop_times_ms = [(t-t_)*1000 for t_,t in zip(time_arr[0:-1], time_arr[1:])]\n",
    "\n",
    "# bb_arr = np.array(bb_arr)\n",
    "# indexes = bb_arr != -1\n",
    "\n",
    "# time_arr = np.array(time_arr)[indexes]\n",
    "# bb_arr = bb_arr[indexes]\n",
    "# bb_angles_arr = np.array(bb_angles_arr)[indexes]\n",
    "# # loop_times_ms = np.array(loop_times_ms)[indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_angles = [(xL+xR)/2 for ((xL, _), (xR, _)) in bb_angles_arr]\n",
    "# plt.plot(time_arr, x_angles, label='estimated angle from nn [degrees]')\n",
    "# plt.grid(); plt.legend(); fig = plt.gcf(); fig.set_size_inches(18.5, 3, forward=True); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plt.plot(loop_times_ms, label='time per loop [ms] (%d ms total)' % sum(loop_times_ms))\n",
    "# plt.legend(); plt.grid(); fig = plt.gcf(); fig.set_size_inches(18.5, 3, forward=True); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IC.image_queue.qsize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IC.dict_queue.qsize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# im = IC.image_queue.get()\n",
    "# PIL.Image.fromarray(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for displaying images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import PIL.Image, PIL.ImageDraw, PIL.ImageFont\n",
    "\n",
    "# def display_image(output_dict, class_of_interest, frame, labels, capture_screenshots=False):\n",
    "#     # Print the results (each image/frame may have multiple objects)\n",
    "#     for i in range(output_dict['num_detections']):\n",
    "        \n",
    "#         if (output_dict.get('detection_classes_' + str(i)) == class_of_interest):\n",
    "            \n",
    "#             # Extract top-left & bottom-right coordinates of detected objects\n",
    "#             (y1, x1) = output_dict.get('detection_boxes_' + str(i))[0]\n",
    "#             (y2, x2) = output_dict.get('detection_boxes_' + str(i))[1]\n",
    "\n",
    "#             # Prep string to overlay on the image\n",
    "#             display_str = (labels[output_dict.get('detection_classes_%i' % i)]\n",
    "#                            + ': %s%%' % output_dict.get('detection_scores_%i' % i))\n",
    "\n",
    "#             # Overlay bounding boxes, detection class and scores\n",
    "#             frame = draw_bounding_box( \n",
    "#                         y1, x1, y2, x2,\n",
    "#                         frame, display_str=display_str)\n",
    "\n",
    "#     if capture_screenshots:\n",
    "#         img = PIL.Image.fromarray(frame)\n",
    "#         img.save('captures/photo_%s.jpg' % cur_time)\n",
    "\n",
    "#     # If a display is available, show image on which inference was performed\n",
    "#     if 'DISPLAY' in os.environ:\n",
    "#         img.show()\n",
    "\n",
    "# # ****************************************************************************\n",
    "# # Copyright(c) 2017 Intel Corporation. \n",
    "# # License: MIT See LICENSE file in root directory.\n",
    "# # ****************************************************************************\n",
    "\n",
    "# # Utilities to help visualize the output from\n",
    "# # Intel® Movidius™ Neural Compute Stick (NCS)\n",
    "# def draw_bounding_box(y1, x1, y2, x2, \n",
    "#                       img, \n",
    "#                       thickness=4, \n",
    "#                       color=(255, 255, 0),\n",
    "#                       display_str=()):\n",
    "#     \"\"\" draw a bounding box on an image to help visualise the nn output\n",
    "    \n",
    "#     Inputs\n",
    "#         (x1, y1)  = Top left corner of the bounding box\n",
    "#         (x2, y2)  = Bottom right corner of the bounding box\n",
    "#         img       = Image/frame represented as numpy array\n",
    "#         thickness = Thickness of the bounding box's outline\n",
    "#         color     = Color of the bounding box's outline\n",
    "#     \"\"\"\n",
    "#     img = PIL.Image.fromarray(img)\n",
    "#     draw = PIL.ImageDraw.Draw(img)\n",
    "\n",
    "#     for x in range(0, thickness):\n",
    "#         draw.rectangle([(x1-x, y1-x), (x2-x, y2-x)], outline=color)\n",
    "\n",
    "#     font = PIL.ImageFont.load_default()\n",
    "#     draw.text((x1, y1), display_str, font=font)\n",
    "\n",
    "#     return np.array(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
